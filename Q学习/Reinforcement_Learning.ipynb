{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hemu2014/python-ML/blob/main/Q%E5%AD%A6%E4%B9%A0/Reinforcement_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ADWvu7NKN2r"
      },
      "source": [
        "##Reinforcement Learning\n",
        "The next and final topic in this course covers *Reinforcement Learning*. This technique is different than many of the other machine learning techniques we have seen earlier and has many applications in training agents (an AI) to interact with enviornments like games. Rather than feeding our machine learning model millions of examples we let our model come up with its own examples by exploring an enviornemt. The concept is simple. Humans learn by exploring and learning from mistakes and past experiences so let's have our computer do the same.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGCR3JWQLaQb"
      },
      "source": [
        "###Terminology\n",
        "Before we dive into explaining reinforcement learning we need to define a few key peices of terminology.\n",
        "\n",
        "**Enviornemt** In reinforcement learning tasks we have a notion of the enviornment. This is what our *agent* will explore. An example of an enviornment in the case of training an AI to play say a game of mario would be the level we are training the agent on.\n",
        "\n",
        "**Agent** an agent is an entity that is exploring the enviornment. Our agent will interact and take different actions within the enviornment. In our mario example the mario character within the game would be our agent.\n",
        "\n",
        "**State** always our agent will be in what we call a *state*. The state simply tells us about the status of the agent. The most common example of a state is the location of the agent within the enviornment. Moving locations would change the agents state.\n",
        "\n",
        "**Action** any interaction between the agent and enviornment would be considered an action. For example, moving to the left or jumping would be an action. An action may or may not change the current *state* of the agent. In fact, the act of doing nothing is an action as well! The action of say not pressing a key if we are using our mario example.\n",
        "\n",
        "**Reward** every action that our agent takes will result in a reward of some magnitude (positive or negative). The goal of our agent will be to maximize its reward in an enviornment. Sometimes the reward will be clear, for example if an agent performs an action which increases their score in the enviornment we could say they've recieved a positive reward. If the agent were to perform an action which results in them losing score or possibly dying in the enviornment then they would recieve a negative reward.\n",
        "\n",
        "The most important part of reinforcement learning is determing how to reward the agent. After all, the goal of the agent is to maximize its rewards. This means we should reward the agent appropiatly such that it reaches the desired goal.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoOJy9s4ZJJt"
      },
      "source": [
        "###Q-Learning\n",
        "Now that we have a vague idea of how reinforcement learning works it's time to talk about a specific technique in reinforcement learning called *Q-Learning*.\n",
        "\n",
        "Q-Learning is a simple yet quite powerful technique in machine learning that involves learning a matrix of action-reward values. This matrix is often reffered to as a Q-Table or Q-Matrix. The matrix is in shape (number of possible states, number of possible actions) where each value at matrix[n, m] represents the agents expected reward given they are in state n and take action m. The Q-learning algorithm defines the way we update the values in the matrix and decide what action to take at each state. The idea is that after a succesful training/learning of this Q-Table/matrix we can determine the action an agent should take in any state by looking at that states row in the matrix and taking the maximium value column as the action.\n",
        "\n",
        "**Consider this example.**\n",
        "\n",
        "Let's say A1-A4 are the possible actions and we have 3 states represented by each row (state 1 - state 3).\n",
        "\n",
        "| A1  | A2  | A3  | A4  |\n",
        "|:--: |:--: |:--: |:--: |\n",
        "|  0  |  0  | 10  |  5  |\n",
        "|  5  | 10  |  0  |  0  |\n",
        "| 10  |  5  |  0  |  0  |\n",
        "\n",
        "If that was our Q-Table/matrix then the following would be the preffered actions in each state.\n",
        "\n",
        "> State 1: A3\n",
        "\n",
        "> State 2: A2\n",
        "\n",
        "> State 3: A1\n",
        "\n",
        "We can see that this is because the values in each of those columns are the highest for those states!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5uLpN1yemTx"
      },
      "source": [
        "###Learning the Q-Table\n",
        "So that's simple, right? Now how do we create this table and find those values. Well this is where we will dicuss how the Q-Learning algorithm updates the values in our Q-Table.\n",
        "\n",
        "I'll start by noting that our Q-Table starts of with all 0 values. This is because the agent has yet to learn anything about the enviornment.\n",
        "\n",
        "Our agent learns by exploring the enviornment and observing the outcome/reward from each action it takes in each state. But how does it know what action to take in each state? There are two ways that our agent can decide on which action to take.\n",
        "1. Randomly picking a valid action\n",
        "2. Using the current Q-Table to find the best action.\n",
        "\n",
        "Near the beginning of our agents learning it will mostly take random actions in order to explore the enviornment and enter many different states. As it starts to explore more of the enviornment it will start to gradually rely more on it's learned values (Q-Table) to take actions. This means that as our agent explores more of the enviornment it will develop a better understanding and start to take \"correct\" or better actions more often. It's important that the agent has a good balance of taking random actions and using learned values to ensure it does get trapped in a local maximum.\n",
        "\n",
        "After each new action our agent wil record the new state (if any) that it has entered and the reward that it recieved from taking that action. These values will be used to update the Q-Table. The agent will stop taking new actions only once a certain time limit is reached or it has acheived the goal or reached the end of the enviornment.\n",
        "\n",
        "####Updating Q-Values\n",
        "The formula for updating the Q-Table after each action is as follows:\n",
        "> $ Q[state, action] = Q[state, action] + \\alpha * (reward + \\gamma * max(Q[newState, :]) - Q[state, action]) $\n",
        "\n",
        "- $\\alpha$ stands for the **Learning Rate**\n",
        "\n",
        "- $\\gamma$ stands for the **Discount Factor**\n",
        "\n",
        "####Learning Rate $\\alpha$\n",
        "The learning rate $\\alpha$ is a numeric constant that defines how much change is permitted on each QTable update. A high learning rate means that each update will introduce a large change to the current state-action value. A small learning rate means that each update has a more subtle change. Modifying the learning rate will change how the agent explores the enviornment and how quickly it determines the final values in the QTable.\n",
        "\n",
        "####Discount Factor $\\gamma$\n",
        "Discount factor also know as gamma ($\\gamma$) is used to balance how much focus is put on the current and future reward. A high discount factor means that future rewards will be considered more heavily.\n",
        "\n",
        "<br/>\n",
        "<p>To perform updates on this table we will let the agent explpore the enviornment for a certain period of time and use each of its actions to make an update. Slowly we should start to notice the agent learning and choosing better actions. </p>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwIl0sJgmu4D"
      },
      "source": [
        "##Q-Learning Example\n",
        "For this example we will use the Q-Learning algorithm to train an agent to navigate a popular enviornment from the [Open AI Gym](https://gym.openai.com/). The Open AI Gym was developed so programmers could practice machine learning using unique enviornments. Intersting fact, Elon Musk is one of the founders of OpenAI!\n",
        "\n",
        "Let's start by looking at what Open AI Gym is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSETF0zqokYr"
      },
      "source": [
        "import gym   # all you have to do to import and use open ai gym!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(gym.__version__)"
      ],
      "metadata": {
        "id": "fIC_vBLt16Zr",
        "outputId": "f748d8cc-378e-45e2-e26f-7552252e71a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.25.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show gym"
      ],
      "metadata": {
        "id": "mSHi2K2M1-BD",
        "outputId": "9fc91632-fcde-4a48-b0ed-5bc52dc70cae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: gym\n",
            "Version: 0.25.2\n",
            "Summary: Gym: A universal API for reinforcement learning environments\n",
            "Home-page: https://www.gymlibrary.ml/\n",
            "Author: Gym Community\n",
            "Author-email: jkterry@umd.edu\n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: cloudpickle, gym-notices, numpy\n",
            "Required-by: dopamine_rl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cH3AmCzotO1"
      },
      "source": [
        "Once you import gym you can load an enviornment using the line ```gym.make(\"enviornment\")```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKN1ScBco3dp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1cc8b4b-3fa7-4ce0-943c-dfae503f3f8d"
      },
      "source": [
        "env = gym.make('FrozenLake-v1', render_mode='human')  # we are going to use the FrozenLake enviornment"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SvSlmVwo8cY"
      },
      "source": [
        "There are a few other commands that can be used to interact and get information about the enviornment."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(env.observation_space)\n",
        "print(env.action_space)"
      ],
      "metadata": {
        "id": "twFzFaeh4rUT",
        "outputId": "fcba63c9-a1b6-4db1-8b61-e544cea4aa73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discrete(16)\n",
            "Discrete(4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FF3icIeapFct",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "129e5845-dbcf-479a-c066-84f0332463f8"
      },
      "source": [
        "print(env.observation_space.n)   # get number of states\n",
        "print(env.action_space.n)   # get number of actions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16\n",
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###重置环境"
      ],
      "metadata": {
        "id": "CRGFyn1240SD"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lc9cwp03pQVn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aef6a421-04e2-4b89-d298-573ecb43b281"
      },
      "source": [
        "env.reset()  # reset enviornment to default state"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####获取一个随机动作"
      ],
      "metadata": {
        "id": "_XBW2_Vz4_Vr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sngyjPDapUt7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8920e1f0-96cf-49ff-a4be-b9b9ede467cb"
      },
      "source": [
        "action = env.action_space.sample()  # get a random action\n",
        "print(action)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy --upgrade"
      ],
      "metadata": {
        "id": "Cf1s98mJ5iWb",
        "outputId": "5d2f0b15-1396-41bd-d765-4e4ee5275597",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.2.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####env.step(action)\n",
        "执行给定的动作，返回以下信息：\n",
        "\n",
        "* new_state：执行动作后的新状态（如 1 表示向右移动后的格子）。\n",
        "\n",
        "* reward：即时奖励（FrozenLake 中只有到达目标时 reward=1，其他情况为 0）。\n",
        "\n",
        "* done：是否为终止状态（True 表示到达目标或掉入冰洞）。\n",
        "\n",
        "* info：额外信息（如概率等调试数据，通常为空字典 {}）。"
      ],
      "metadata": {
        "id": "x5rTysPE9wqM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeEfi8xypXya",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ff438b5-c998-4353-98ad-5be1827763f1"
      },
      "source": [
        "new_state, reward, done, info = env.step(action)  # take action, notice it returns information about the action\n",
        "print(new_state)  # 新状态\n",
        "print(reward)     # 即时奖励\n",
        "print(done)       # 是否终止(决定下一步是否执行)\n",
        "print(info)       # 额外信息"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "0.0\n",
            "False\n",
            "{'prob': 0.3333333333333333}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####渲染环境\n",
        ".render() 是属于环境对象的一个函数。当调用时，它会生成环境当前状态的视觉表示（一个 GUI - 图形用户界面），显示代理的位置和冻结湖布局，包括 'S'（起点）、'F'（冻结）、'H'（坑）和 'G'（目标）元素。"
      ],
      "metadata": {
        "id": "RXqwR1PD59Kj"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1W3D81ipdaS"
      },
      "source": [
        "env.render()   # render the GUI for the enviornment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmW6HAbQp01f"
      },
      "source": [
        "###Frozen Lake Enviornment\n",
        "Now that we have a basic understanding of how the gym enviornment works it's time to discuss the specific problem we will be solving.\n",
        "\n",
        "The enviornment we loaded above ```FrozenLake-v0``` is one of the simplest enviornments in Open AI Gym. The goal of the agent is to navigate a frozen lake and find the Goal without falling through the ice (render the enviornment above to see an example).\n",
        "\n",
        "There are:\n",
        "- 16 states (one for each square)\n",
        "- 4 possible actions (LEFT, RIGHT, DOWN, UP)\n",
        "- 4 different types of blocks (F: frozen, H: hole, S: start, G: goal)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlWoK75ZrK2b"
      },
      "source": [
        "###Building the Q-Table\n",
        "The first thing we need to do is build an empty Q-Table that we can use to store and update our values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r767K4s0rR2p"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "env = gym.make('FrozenLake-v1', render_mode='human')\n",
        "STATES = env.observation_space.n  #获取环境状态信息\n",
        "ACTIONS = env.action_space.n    #获取动作信息"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####构建一个Q学习表\n",
        "使用numpy构建一个（states, actions）相应的0矩阵"
      ],
      "metadata": {
        "id": "aJAdPskK8h6k"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAzMWGatrVIk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "177691d6-35fc-44a2-800b-43224d6f3f01"
      },
      "source": [
        "Q = np.zeros((STATES, ACTIONS))  # create a matrix with all 0 values\n",
        "print(Q)\n",
        "print(Q.shape)\n",
        "print(Q.ndim)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n",
            "(16, 4)\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vc_h8tLSrpmc"
      },
      "source": [
        "###Constants\n",
        "As we discussed we need to define some constants that will be used to update our Q-Table and tell our agent when to stop training."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EPISODES = 2000：训练 2000 次完整游戏（从起点到终点/冰洞）。\n",
        "\n",
        "MAX_STEPS = 100：如果智能体在 100 步内未完成任务，强制结束当前回合。\n",
        "\n",
        "LEARNING_RATE (α)：\n",
        "\n",
        "* 取值范围 0 < α ≤ 1，控制新信息对 Q 值的影响。\n",
        "\n",
        "* α=0.81 表示新信息占 81% 权重，旧 Q 值占 19%。\n",
        "\n",
        "GAMMA (γ)：\n",
        "\n",
        "* 取值范围 0 ≤ γ < 1，衡量未来奖励的重要性。\n",
        "\n",
        "* γ=0.96 表示智能体更关注长期回报（接近 1 时更重视未来）。"
      ],
      "metadata": {
        "id": "vZFMbqEp_C2U"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FQapdnnr6P1"
      },
      "source": [
        "# 训练的总回合数\n",
        "EPISODES = 2000 # how many times to run the enviornment from the beginning\n",
        "#\n",
        "## 每个回合的最大步数（防止无限循环）\n",
        "MAX_STEPS = 100  # max number of steps allowed for each run of enviornment\n",
        "\n",
        "LEARNING_RATE = 0.81  # learning rate\n",
        "GAMMA = 0.96\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxrAj91rsMfm"
      },
      "source": [
        "###Picking an Action\n",
        "Remember that we can pick an action using one of two methods:\n",
        "1. Randomly picking a valid action\n",
        "2. Using the current Q-Table to find the best action.\n",
        "\n",
        "Here we will define a new value $\\epsilon$ that will tell us the probabillity of selecting a random action. This value will start off very high and slowly decrease as the agent learns more about the enviornment."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "np.random.uniform(low=0.0, high=1.0, size=None)\n",
        "low: 随机数的范围的下限（包含）。默认为 0.0。\n",
        "high: 随机数的范围的上限（不包含）。默认为 1.0。\n",
        "size: 输出的形状。如果给定整数，则返回该大小的一维数组；如果给定元组，则返回该形状的数组。默认为 None，表示返回一个单一的浮点数。\n",
        "'''\n",
        "print(np.random.uniform(0, 1))\n",
        "print(np.random.uniform(0, 1, 3))\n",
        "print(np.random.uniform(0, 1, (3,2,2)))  # 生成一个矩阵"
      ],
      "metadata": {
        "id": "swtIMDm9CWH1",
        "outputId": "2bb3b7c8-c0d0-4b7f-a852-2eca474eab3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.2483560822932115\n",
            "[0.40394046 0.57306069 0.66622558]\n",
            "[[[0.00199453 0.25604968]\n",
            "  [0.59810096 0.13475361]]\n",
            "\n",
            " [[0.76290595 0.77993781]\n",
            "  [0.70525648 0.13709385]]\n",
            "\n",
            " [[0.31068186 0.76002014]\n",
            "  [0.58822745 0.08794071]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "epsilon 是一个动态衰减的参数，控制探索的概率：\n",
        "\n",
        "* 初始值 0.9：表示智能体在训练初期有 90% 的概率随机选择动作（探索环境），10% 的概率选择当前 Q 表认为最优的动作（利用已有知识）。\n",
        "\n",
        "* 后续通常会衰减：例如每回合减少 0.001，逐步降低探索比例，让智能体后期更依赖学到的策略。"
      ],
      "metadata": {
        "id": "SPNjW8VaBjat"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUAQVyX0sWDb"
      },
      "source": [
        "epsilon = 0.9  # start with a 90% chance of picking a random action\n",
        "\n",
        "# code to pick action\n",
        " # 生成一个 [0,1) 的随机数，若小于 epsilon 则随机探索\n",
        "if np.random.uniform(0, 1) < epsilon:  # we will check if a randomly selected value is less than epsilon.\n",
        "    action = env.action_space.sample()  # take random action\n",
        "else:\n",
        "    # 有1-epsilon的概率选择学习表， 取当前state的对应action的最大Q值的索引，也就是执行相应的动作\n",
        "    #  # 选择当前状态下 Q 值最大的动作（利用）\n",
        "    action = np.argmax(Q[state, :])  # use Q table to pick best action based on current values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5n-i0B7Atige"
      },
      "source": [
        "###Updating Q Values\n",
        "The code below implements the formula discussed above."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####更新当前Q值"
      ],
      "metadata": {
        "id": "N6kTosXDBH8l"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9r7R1W6Qtnh8"
      },
      "source": [
        "Q[state, action] = Q[state, action] + LEARNING_RATE * (reward + GAMMA * np.max(Q[new_state, :]) - Q[state, action])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__afaD62uh8G"
      },
      "source": [
        "###Putting it Together\n",
        "Now that we know how to do some basic things we can combine these together to create our Q-Learning algorithm,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGiYCiNuutHz"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "env = gym.make('FrozenLake-v1', render_mode='human')\n",
        "STATES = env.observation_space.n  #获取环境状态的数量\n",
        "ACTIONS = env.action_space.n   # 获取动作的数量\n",
        "\n",
        " #使用numpy构建一个（states, actions）相应的0矩阵\n",
        "Q = np.zeros((STATES, ACTIONS))\n",
        "\n",
        "EPISODES = 1000 # how many times to run the enviornment from the beginning\n",
        "MAX_STEPS = 100  # max number of steps allowed for each run of enviornment\n",
        "\n",
        "LEARNING_RATE = 0.81  # learning rate\n",
        "GAMMA = 0.96\n",
        "\n",
        "RENDER = False # if you want to see training set to true\n",
        "# epsilon 是一个动态衰减的参数，控制探索的概率：\n",
        "epsilon = 0.9\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "source": [
        "!pip install --upgrade numpy"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "wfBvMozJJnTG",
        "outputId": "f6173c0d-06ae-437c-d499-a5cacdc0f28b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.2.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFRtn5dUu5ZI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c97ee9b-6f4a-4d22-899e-b8123a6c7caa"
      },
      "source": [
        "#作用：存储每次训练回合（episode）结束时的奖励（如到达目标 G 时 reward=1，掉入冰洞 H 时 reward=0）。\n",
        "#后续用途：计算平均奖励，评估训练效果。\n",
        "rewards = []  #  # 用于记录每个回合（episode）的最终奖励\n",
        "for episode in range(EPISODES):\n",
        "\n",
        "  state = env.reset()\n",
        "  for _ in range(MAX_STEPS):\n",
        "\n",
        "    if RENDER:\n",
        "      env.render()\n",
        "\n",
        "    if np.random.uniform(0, 1) < epsilon:\n",
        "      action = env.action_space.sample()\n",
        "    else:\n",
        "      action = np.argmax(Q[state, :])\n",
        "\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "   # 更新相应的学习表的Q值\n",
        "    Q[state, action] = Q[state, action] + LEARNING_RATE * (reward + GAMMA * np.max(Q[next_state, :]) - Q[state, action])\n",
        "\n",
        "    # 将新的状态(也就是新的位置)赋值给 state, 下个循序使用\n",
        "    state = next_state\n",
        "   # 如果done为TRUE，添加到相应的列表中\n",
        "    if done:\n",
        "      rewards.append(reward)  # # 记录回合奖励\n",
        "      epsilon -= 0.001  #  # 衰减探索概率\n",
        "      break  # reached goal\n",
        "\n",
        "print(Q)\n",
        "print(rewards)\n",
        "print(len(rewards))\n",
        "print(f\"Average reward: {sum(rewards)/len(rewards)}:\")  # 平均回报值\n",
        "# and now we can see our Q values!"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[3.49324075e-01 6.61151520e-02 6.40969466e-02 7.03414654e-02]\n",
            " [3.81711042e-02 2.03170179e-02 3.09279795e-02 2.63711116e-01]\n",
            " [2.99367699e-02 5.59102158e-02 2.20404771e-02 3.30042372e-02]\n",
            " [8.05834712e-03 8.57693409e-03 5.00890089e-04 3.33722891e-02]\n",
            " [3.94390041e-01 5.02716504e-02 4.22499541e-02 1.04408522e-02]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [3.26593115e-03 3.67103044e-03 1.88413739e-02 3.20839307e-03]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [5.13717073e-03 1.37653400e-02 1.95442696e-02 5.60048298e-01]\n",
            " [2.25644956e-02 6.01214809e-01 2.60354916e-02 4.98423115e-02]\n",
            " [1.96948037e-01 1.08634230e-02 5.38829968e-03 1.03152218e-02]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [6.97216966e-02 1.08169278e-01 7.62218289e-01 9.98189871e-02]\n",
            " [5.35261915e-01 8.97791729e-01 5.06430990e-01 5.42673797e-01]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]\n",
            "1000\n",
            "Average reward: 0.137:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zo-tNznd65US",
        "outputId": "25719aaf-75fe-48f4-dd79-9e240f3c8c01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        }
      },
      "source": [
        "# we can plot the training progress and see how the agent improved\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def get_average(values):\n",
        "  return sum(values)/len(values)\n",
        "\n",
        "avg_rewards = []\n",
        "for i in range(0, len(rewards), 100):\n",
        "  avg_rewards.append(get_average(rewards[i:i+100]))\n",
        "\n",
        "plt.plot(avg_rewards)\n",
        "plt.ylabel('average reward')\n",
        "plt.xlabel('episodes (100\\'s)')\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gy4YH2m9s1ww"
      },
      "source": [
        "##Sources\n",
        "1. Violante, Andre. “Simple Reinforcement Learning: Q-Learning.” Medium, Towards Data Science, 1 July 2019, https://towardsdatascience.com/simple-reinforcement-learning-q-learning-fcddc4b6fe56.\n",
        "\n",
        "2. Openai. “Openai/Gym.” GitHub, https://github.com/openai/gym/wiki/FrozenLake-v0."
      ]
    }
  ]
}