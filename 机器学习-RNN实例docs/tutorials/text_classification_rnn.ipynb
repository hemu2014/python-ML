{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hemu2014/python-ML/blob/main/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-RNN%E5%AE%9E%E4%BE%8Bdocs/tutorials/text_classification_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hX4n9TsbGw-f"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0nbI5DtDGw-i"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TnJztDZGw-n"
      },
      "source": [
        "# Text classification with an RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfN3bMR5Gw-o"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/text/tutorials/text_classification_rnn\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/text_classification_rnn.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/text/blob/master/docs/tutorials/text_classification_rnn.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/text/docs/tutorials/text_classification_rnn.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUWearf0Gw-p"
      },
      "source": [
        "This text classification tutorial trains a [recurrent neural network](https://developers.google.com/machine-learning/glossary/#recurrent_neural_network) on the [IMDB large movie review dataset](http://ai.stanford.edu/~amaas/data/sentiment/) for sentiment analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2VQo4bajwUU"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "z682XYsrjkY9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "tfds.disable_progress_bar() #调用tfds.disable_progress_bar()函数来禁用TensorFlow Datasets的下载或加载数据时显示的进度条。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rXHa-w9JZhb"
      },
      "source": [
        "Import `matplotlib` and create a helper function to plot graphs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Mp1Z7P9pYRSK"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_graphs(history, metric):\n",
        "  plt.plot(history.history[metric])\n",
        "  plt.plot(history.history['val_'+metric], '')\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(metric)\n",
        "  plt.legend([metric, 'val_'+metric])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###设置输入流水线"
      ],
      "metadata": {
        "id": "N3s69EgK61QT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRmMubr0jrE2"
      },
      "source": [
        "## Setup input pipeline\n",
        "\n",
        "\n",
        "The IMDB large movie review dataset is a *binary classification* dataset—all the reviews have either a *positive* or *negative* sentiment.\n",
        "\n",
        "Download the dataset using [TFDS](https://www.tensorflow.org/datasets). See the [loading text tutorial](https://www.tensorflow.org/tutorials/load_data/text) for details on how to load this sort of data manually.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SHRwRoP2nVHX",
        "outputId": "c952f315-9d99-441e-fb48-0e95c05e9b46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n每个样本是一个元组 (text, label)。\\ntext 是字符串类型的影评（dtype=tf.string）。\\nlabel 是整型标签（0 或 1，对应负面/正面评价，dtype=tf.int64）。\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "dataset, info = tfds.load('imdb_reviews', with_info=True,\n",
        "                          as_supervised=True)\n",
        "train_dataset, test_dataset = dataset['train'], dataset['test']\n",
        "\n",
        "train_dataset.element_spec  #查看训练集的数据结构（即每个样本的输入和标签的 Tensor 类型和形状）。\n",
        "\n",
        "'''\n",
        "每个样本是一个元组 (text, label)。\n",
        "text 是字符串类型的影评（dtype=tf.string）。\n",
        "label 是整型标签（0 或 1，对应负面/正面评价，dtype=tf.int64）。\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info.features['text']"
      ],
      "metadata": {
        "id": "9mTN-mjFEjfQ",
        "outputId": "f21d01da-78dc-4049-ba73-31042c2941cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(shape=(), dtype=string)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(info.description)"
      ],
      "metadata": {
        "id": "TfU96KWK-7Yy",
        "outputId": "4c72b72a-4b2c-4de3-a2bf-c398e0adb73c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Large Movie Review Dataset. This is a dataset for binary sentiment\n",
            "classification containing substantially more data than previous benchmark\n",
            "datasets. We provide a set of 25,000 highly polar movie reviews for training,\n",
            "and 25,000 for testing. There is additional unlabeled data for use as well.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(info)"
      ],
      "metadata": {
        "id": "ciI_MNwp-d8K",
        "outputId": "c2c40214-1628-4194-e8b1-c03b42bbbcef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tfds.core.DatasetInfo(\n",
            "    name='imdb_reviews',\n",
            "    full_name='imdb_reviews/plain_text/1.0.0',\n",
            "    description=\"\"\"\n",
            "    Large Movie Review Dataset. This is a dataset for binary sentiment\n",
            "    classification containing substantially more data than previous benchmark\n",
            "    datasets. We provide a set of 25,000 highly polar movie reviews for training,\n",
            "    and 25,000 for testing. There is additional unlabeled data for use as well.\n",
            "    \"\"\",\n",
            "    config_description=\"\"\"\n",
            "    Plain text\n",
            "    \"\"\",\n",
            "    homepage='http://ai.stanford.edu/~amaas/data/sentiment/',\n",
            "    data_dir='/root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0',\n",
            "    file_format=tfrecord,\n",
            "    download_size=80.23 MiB,\n",
            "    dataset_size=129.83 MiB,\n",
            "    features=FeaturesDict({\n",
            "        'label': ClassLabel(shape=(), dtype=int64, num_classes=2),\n",
            "        'text': Text(shape=(), dtype=string),\n",
            "    }),\n",
            "    supervised_keys=('text', 'label'),\n",
            "    disable_shuffling=False,\n",
            "    nondeterministic_order=False,\n",
            "    splits={\n",
            "        'test': <SplitInfo num_examples=25000, num_shards=1>,\n",
            "        'train': <SplitInfo num_examples=25000, num_shards=1>,\n",
            "        'unsupervised': <SplitInfo num_examples=50000, num_shards=1>,\n",
            "    },\n",
            "    citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
            "      author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
            "      title     = {Learning Word Vectors for Sentiment Analysis},\n",
            "      booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
            "      month     = {June},\n",
            "      year      = {2011},\n",
            "      address   = {Portland, Oregon, USA},\n",
            "      publisher = {Association for Computational Linguistics},\n",
            "      pages     = {142--150},\n",
            "      url       = {http://www.aclweb.org/anthology/P11-1015}\n",
            "    }\"\"\",\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(train_dataset))"
      ],
      "metadata": {
        "id": "X7JBNpKe-QBC",
        "outputId": "f85cb645-9f32-4c36-a3bb-1f478a4b7e20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'tensorflow.python.data.ops.prefetch_op._PrefetchDataset'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWA4c2ir7g6p"
      },
      "source": [
        "Initially this returns a dataset of (text, label pairs):\n",
        "\n",
        "take(1) 的含义\n",
        "作用：从数据集中提取前1个元素（注意是元素，不是批次）。\n",
        "\n",
        "关键点：\n",
        "\n",
        "* take(N) 是按样本数量（而非批次数量）操作的。\n",
        "\n",
        "* 如果数据集是未分批次的（即每个元素是单个样本），take(1) 就返回1条数据。\n",
        "\n",
        "* 如果数据集已分批次（如 batch(32)），take(1) 会返回1个批次（含32条数据）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "vd4_BGKyurao",
        "outputId": "131ffd1a-1a13-41a5-cd85-6b1cab3d3f88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text:  b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\"\n",
            "text:  This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\n",
            "label:  0\n",
            "text:  b'I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.'\n",
            "text:  I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.\n",
            "label:  0\n",
            "text:  b'Mann photographs the Alberta Rocky Mountains in a superb fashion, and Jimmy Stewart and Walter Brennan give enjoyable performances as they always seem to do. <br /><br />But come on Hollywood - a Mountie telling the people of Dawson City, Yukon to elect themselves a marshal (yes a marshal!) and to enforce the law themselves, then gunfighters battling it out on the streets for control of the town? <br /><br />Nothing even remotely resembling that happened on the Canadian side of the border during the Klondike gold rush. Mr. Mann and company appear to have mistaken Dawson City for Deadwood, the Canadian North for the American Wild West.<br /><br />Canadian viewers be prepared for a Reefer Madness type of enjoyable howl with this ludicrous plot, or, to shake your head in disgust.'\n",
            "text:  Mann photographs the Alberta Rocky Mountains in a superb fashion, and Jimmy Stewart and Walter Brennan give enjoyable performances as they always seem to do. <br /><br />But come on Hollywood - a Mountie telling the people of Dawson City, Yukon to elect themselves a marshal (yes a marshal!) and to enforce the law themselves, then gunfighters battling it out on the streets for control of the town? <br /><br />Nothing even remotely resembling that happened on the Canadian side of the border during the Klondike gold rush. Mr. Mann and company appear to have mistaken Dawson City for Deadwood, the Canadian North for the American Wild West.<br /><br />Canadian viewers be prepared for a Reefer Madness type of enjoyable howl with this ludicrous plot, or, to shake your head in disgust.\n",
            "label:  0\n"
          ]
        }
      ],
      "source": [
        "for example, label in train_dataset.take(3):\n",
        "  print('text: ', example.numpy()) #文本会以字节形式（b'...'）显示。\n",
        "  print('text: ', example.numpy().decode('utf-8')) #如需显示普通字符串\n",
        "  print('label: ', label.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2qVJzcEluH_"
      },
      "source": [
        "Next shuffle the data for training and create batches of these `(text, label)` pairs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dDsCaZCDYZgm"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "VznrltNOnUc5"
      },
      "outputs": [],
      "source": [
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "'''\n",
        ".prefetch(tf.data.AUTOTUNE)\n",
        "作用：异步预取数据，即在模型训练当前批次时，后台提前加载下一批次数据。\n",
        "参数：tf.data.AUTOTUNE：由TensorFlow自动选择最优的预取缓冲区大小。\n",
        "为什么需要：避免数据加载成为训练瓶颈（即GPU等模型计算设备无需等待数据加载）。\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 取前三个数据，**已经分批次了**"
      ],
      "metadata": {
        "id": "HabIjzimB_s5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for example, label in train_dataset.take(1):\n",
        "  print(len(label.numpy()))\n",
        "  for i in range(3):\n",
        "    print('text: ', example.numpy()[i]) #文本会以字节形式（b'...'）显示。\n",
        "    print('text: ', example.numpy()[i].decode('utf-8')) #如需显示普通字符串\n",
        "    print('label: ', label.numpy()[i])"
      ],
      "metadata": {
        "id": "fKYq6ZJCCJBp",
        "outputId": "54c93b02-e698-48a3-b54c-0fa65ef1cdc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n",
            "text:  b\"I searched for this movie for years, apparently it ain't available here in the States so bought me a copy off Ebay.<br /><br />Four young hunters and three of their girlfriends venture into the woods searching for a bear that apparently has killed several campers. What they find is an ex-Vietnam vet gone crazy (he kills some of his victims using a glove with long metal finger nails a la Freddy Krueger). As soon as the night falls, one of the girls goes for a walk after a brief argument with her boyfriend, she gets killed. After one of the group finds her body, they all hide in their tents waiting for daylight. Once the sun comes up, all of them try and make it out, but fall victim one by one.<br /><br />Seven bodies, not a lot of gore, but a couple of good murders, especially the girls'deaths. The guys get killed in somewhat bloodless ways (blown up in car, shot to death, knife through head). <br /><br />Overall, INFERNAL TRAP is a nice slasher film from the late 80's. Nothing new, just well acted, fast paced and some pretty ladies. 10 out of 10.\"\n",
            "text:  I searched for this movie for years, apparently it ain't available here in the States so bought me a copy off Ebay.<br /><br />Four young hunters and three of their girlfriends venture into the woods searching for a bear that apparently has killed several campers. What they find is an ex-Vietnam vet gone crazy (he kills some of his victims using a glove with long metal finger nails a la Freddy Krueger). As soon as the night falls, one of the girls goes for a walk after a brief argument with her boyfriend, she gets killed. After one of the group finds her body, they all hide in their tents waiting for daylight. Once the sun comes up, all of them try and make it out, but fall victim one by one.<br /><br />Seven bodies, not a lot of gore, but a couple of good murders, especially the girls'deaths. The guys get killed in somewhat bloodless ways (blown up in car, shot to death, knife through head). <br /><br />Overall, INFERNAL TRAP is a nice slasher film from the late 80's. Nothing new, just well acted, fast paced and some pretty ladies. 10 out of 10.\n",
            "label:  1\n",
            "text:  b\"This movie is not based on the bible. It completely leaves Christ out of the movie. They do not show the rapture or the second coming of Christ. Let alone talk about it. It does not quote from scriptures. The end times are called the great tribulation. The movie does not even show bad times. The seven bowls, seven viles and seven trumpets of judgements are boiled down to a 15 second news cast of the sea changing it's structure. The anti-Christ was killed 3 1/2 years into the tribulation and that is how the movie ended. The only part they got correct was there was two prophets. The did not use there names of course because that would be too close to the truth of scriptures. The worst part of it was I really wanted it to be a good movie. I wanted to take unsaved people to it. I feel that the movie is evil. It is a counterfeit just like everything the devil does. I just hope it does not take away from the upcoming movie based on the left behind books.<br /><br />The second problem with the movie is it was just bad. Bad acting, bad special effects, bad plot and poor character development. I have seen better episodes of Miami vice.\"\n",
            "text:  This movie is not based on the bible. It completely leaves Christ out of the movie. They do not show the rapture or the second coming of Christ. Let alone talk about it. It does not quote from scriptures. The end times are called the great tribulation. The movie does not even show bad times. The seven bowls, seven viles and seven trumpets of judgements are boiled down to a 15 second news cast of the sea changing it's structure. The anti-Christ was killed 3 1/2 years into the tribulation and that is how the movie ended. The only part they got correct was there was two prophets. The did not use there names of course because that would be too close to the truth of scriptures. The worst part of it was I really wanted it to be a good movie. I wanted to take unsaved people to it. I feel that the movie is evil. It is a counterfeit just like everything the devil does. I just hope it does not take away from the upcoming movie based on the left behind books.<br /><br />The second problem with the movie is it was just bad. Bad acting, bad special effects, bad plot and poor character development. I have seen better episodes of Miami vice.\n",
            "label:  0\n",
            "text:  b'In 1978 a phenomenon began. The release of John Carpenter\\'s \"Halloween\" got people queueing around the block to witness the evil that is Michael Meyers. The critics loved it, the world loved it, it was imitated, and has gone down as one of the greatest movies in cinematic history.<br /><br />plot: 15 years after a murder took place, four friends (all females) are babysitting(and having it on with their boyfriends) on Halloween night. After escaping from a hospital the night before, Michael myers Returns to his home town to stalk these people. He murders 3 of them silently and subtlety. He does not speak. He walks slowly. He hides....<br /><br />Only one of the friends escapes, after being saved by Doctor Loomis (Michael\\'s pursuer and doctor) <br /><br />There is one reason why Halloween works so well. Simplicity. We don\\'t know where Michael is, we don\\'t know why he kills, and he frightens us. They\\'re the only reasons why we are afraid.<br /><br />John carpenter wrote the movie, and directed. He builds unbearable tension throughout the story, and scares to such a degree, that sometimes we cannot watch. And the climax is truly startling.<br /><br />As horror, this is essential. It is terrifying and well acted. It is also mysterious. Michael is a force, not a human. A force that cannot be denied.<br /><br />The sequels focused too much around Michael and his \"history\". This movie focuses on the fear of the unknown. Perhaps that\\'s why this thing is a masterpiece.'\n",
            "text:  In 1978 a phenomenon began. The release of John Carpenter's \"Halloween\" got people queueing around the block to witness the evil that is Michael Meyers. The critics loved it, the world loved it, it was imitated, and has gone down as one of the greatest movies in cinematic history.<br /><br />plot: 15 years after a murder took place, four friends (all females) are babysitting(and having it on with their boyfriends) on Halloween night. After escaping from a hospital the night before, Michael myers Returns to his home town to stalk these people. He murders 3 of them silently and subtlety. He does not speak. He walks slowly. He hides....<br /><br />Only one of the friends escapes, after being saved by Doctor Loomis (Michael's pursuer and doctor) <br /><br />There is one reason why Halloween works so well. Simplicity. We don't know where Michael is, we don't know why he kills, and he frightens us. They're the only reasons why we are afraid.<br /><br />John carpenter wrote the movie, and directed. He builds unbearable tension throughout the story, and scares to such a degree, that sometimes we cannot watch. And the climax is truly startling.<br /><br />As horror, this is essential. It is terrifying and well acted. It is also mysterious. Michael is a force, not a human. A force that cannot be denied.<br /><br />The sequels focused too much around Michael and his \"history\". This movie focuses on the fear of the unknown. Perhaps that's why this thing is a masterpiece.\n",
            "label:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 取的时候应该是随机的，因为清洗，shuffle"
      ],
      "metadata": {
        "id": "Lm0YklRJDHQI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "jqkvdcFv41wC",
        "outputId": "2e0572a5-56fe-4395-9742-cc470f5e8222",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "texts:  [b'I wasn\\'t as \"lucky\" as some of the others commenting on this film: i have never seen anything else out of the...shall we say... \"fecund\" mind of Sarno. I agree with many: some of the actresses who spend a lot of time topless and (go-go) dancing are not really that attractive. I kinda liked Fraulein Crank(?)...she was so homely , she was cute! The acting was pretty stale, also, though delivering lines in a second language might have accounted for a lot of that problem. Trying to follow the plot was a major chore: was there one, really? I do heartily agree with one other comment: for a vampire movie, there\\'s not much blood. Yep, if you want GOOD bloodsucking flicks, check out such Hammer classics as \"Horror of Dracula\" and (my personal favourite) \"Brides of Dracula\".<br /><br />The most (unintentionally) humorous part is where the lady doctor gets her clothes torn off by a cloud of bats...which you never SEE!...the bats, I mean.<br /><br />Okay as a time-waster if you happen to catch it on cable here in the Great White North but, for heaven\\'s sake, don\\'t rent it!'\n",
            " b\"I have seen a lot of Saura films and always found amazing the way he assembles music, dance, drama and great cinema in his movies. Ib\\xc3\\xa9ria shows an even better Saura, dealing with multimedia concepts and a more contemporary concept of dance and music. Another thing that called my attention is the fact that, in this movie, dancers and musicians, dance and music, are equally important: the camera shows various aspects of music interpretation, examining not only technical issues but also the emotional experience of playing. The interest of Saura on the bridge between classical and contemporary music and dance is one more ingredient in turning this movie maybe the most aesthetically exciting among his other works. That's why I recommend it strongly to those who love good cinema, good music, good dance, great art.\"\n",
            " b'While many unfortunately passed on, the ballroom scene is still very much alive and carrying on their legacy. Some are still very much alive and quite well, Octavia is more radiant and beautiful than ever, Willi Ninja is very accomplished and gives a great deal of support to the gay community as a whole, Pepper Labeija just passed on last year of natural cause, may she rest in peace. After Anji\\'s passing Carmen became the mother of the house of Xtravaganza (she was in the beach scene) and she is looking more and more lovely as well. Some balls have categories dedicated to those who have passed, may they all rest in peace. There is currently another project underway known as \"How Do I Look?\", you can check out the website at www.howdoilooknyc.org.']\n",
            "<class 'numpy.ndarray'>\n",
            "labels:  [0 1 1]\n"
          ]
        }
      ],
      "source": [
        "for example, label in train_dataset.take(1):\n",
        "  print('texts: ', example.numpy()[:3])\n",
        "  print(type(example.numpy()[:3]))\n",
        "  print('labels: ', label.numpy()[:3])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import tensorflow as tf\n",
        "\n",
        "# 模拟数据集\n",
        "data = tf.data.Dataset.from_tensor_slices(list(\"ABCDEFGHIJ\"))\n",
        "# 设置小缓冲区观察行为\n",
        "shuffled = data.shuffle(buffer_size=3).batch(3)\n",
        "\n",
        "# 查看输出顺序\n",
        "for item in shuffled.take(4): # 取10次观察重复情况\n",
        "\n",
        "  print(item.numpy())"
      ],
      "metadata": {
        "id": "vlaBiQ44GG64",
        "outputId": "5164d5c3-acb6-4e62-8bc0-ec5bd62158e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[b'A' b'B' b'D']\n",
            "[b'E' b'G' b'H']\n",
            "[b'F' b'J' b'C']\n",
            "[b'I']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5eWCo88voPY"
      },
      "source": [
        "## Create the text encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFevcItw15P_"
      },
      "source": [
        "The raw text loaded by `tfds` needs to be processed before it can be used in a model. The simplest way to process text for training is using the `TextVectorization` layer. This layer has many capabilities, but this tutorial sticks to the default behavior.\n",
        "\n",
        "Create the layer, and pass the dataset's text to the layer's `.adapt` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "uC25Lu1Yvuqy",
        "outputId": "4e22fd1f-468a-4ca6-ee1c-a3ac4ec62751",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-941ae86e7c81>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     max_tokens=VOCAB_SIZE) \n\u001b[1;32m      4\u001b[0m \u001b[0;31m#从训练集中提取文本部分（丢弃标签 label），生成一个仅包含文本的新数据集。\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madapt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#遍历所有文本，统计词频并构建词汇表（选择前 VOCAB_SIZE 个高频词）。\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# 测试集/验证集不需要再次 adapt()（应使用相同的词汇表）。\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/layers/preprocessing/text_vectorization.py\u001b[0m in \u001b[0;36madapt\u001b[0;34m(self, data, batch_size, steps)\u001b[0m\n\u001b[1;32m    427\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalize_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/layers/preprocessing/text_vectorization.py\u001b[0m in \u001b[0;36mfinalize_state\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    432\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lookup_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mfinalize_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lookup_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalize_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "VOCAB_SIZE = 1000\n",
        "encoder = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE)\n",
        "#从训练集中提取文本部分（丢弃标签 label），生成一个仅包含文本的新数据集。\n",
        "encoder.adapt(train_dataset.map(lambda text, label: text)) #遍历所有文本，统计词频并构建词汇表（选择前 VOCAB_SIZE 个高频词）。\n",
        "# 测试集/验证集不需要再次 adapt()（应使用相同的词汇表）。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuQzVBbe3Ldu"
      },
      "source": [
        "The `.adapt` method sets the layer's vocabulary. Here are the first 20 tokens. After the padding and unknown tokens they're sorted by frequency:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "tBoyjjWg0Ac9",
        "outputId": "518febb2-f070-4d42-d065-4870a34ff729",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['' '[UNK]' 'the' 'and' 'a' 'of' 'to' 'is' 'in' 'it' 'i' 'this' 'that'\n",
            " 'br' 'was' 'as' 'for' 'with' 'movie' 'but']\n",
            "[False False False False False False False False False False False False\n",
            " False False False False False False  True False]\n",
            "(array([18]),)\n",
            "18\n",
            "Index of '['movie']': 18\n"
          ]
        }
      ],
      "source": [
        "vocab = np.array(encoder.get_vocabulary()) # 获取的词汇表转为numpy数组\n",
        "print(vocab[:20])\n",
        "vocab_test = vocab[:20]\n",
        "# 检查特定词的索引\n",
        "word = ['movie']\n",
        "print(vocab_test == word) #将词汇表中的每个元素与 word 比较，返回一个布尔数组（True/False）。\n",
        "# 类似(array([4]),)  # 注意是元组，包含一个NumPy数组 [4]\n",
        "print(np.where(vocab == word))# 返回满足条件（True）的元素的 索引位置。返回一个元组（即使是一维数组，也会包装成元组）。\n",
        "# 第一个0是取元组的第一个元素得到一个数组， 第二个0访问数组的索引为0的数据\n",
        "print(np.where(vocab == word)[0][0])\n",
        "if word in vocab:\n",
        "    print(f\"Index of '{word}':\", np.where(vocab == word)[0][0])\n",
        "else:\n",
        "    print(f\"'{word}' is out of vocabulary!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###查找多个单词索引"
      ],
      "metadata": {
        "id": "LKJIF9tXQfp-"
      }
    },
    {
      "source": [
        "vocab = np.array(encoder.get_vocabulary()) # 获取的词汇表转为numpy数组\n",
        "print(vocab[:20])\n",
        "vocab_test = vocab[:20]\n",
        "# 检查特定词的索引\n",
        "word = ['movie', 'but']\n",
        "# Use np.isin() to check if elements of 'word' are in 'vocab_test'\n",
        "print(np.isin(vocab_test, word)) # This will return a boolean array\n",
        "# Use np.where() with np.isin() to find the indices\n",
        "print(np.where(np.isin(vocab, word))) # This will return the indices of 'word' elements in 'vocab'\n",
        "# Loop through the words and check if they're in the vocabulary\n",
        "for w in word:\n",
        "    if w in vocab:\n",
        "        print(f\"Index of '{w}':\", np.where(vocab == w)[0][0])\n",
        "    else:\n",
        "        print(f\"'{w}' is out of vocabulary!\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "fkmn7DpZPjN-",
        "outputId": "93730df0-3ee8-4b1c-c088-fb9a618987ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['' '[UNK]' 'the' 'and' 'a' 'of' 'to' 'is' 'in' 'it' 'i' 'this' 'that'\n",
            " 'br' 'was' 'as' 'for' 'with' 'movie' 'but']\n",
            "[False False False False False False False False False False False False\n",
            " False False False False False False  True  True]\n",
            "(array([18, 19]),)\n",
            "Index of 'movie': 18\n",
            "Index of 'but': 19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###构建字典查询"
      ],
      "metadata": {
        "id": "ehm3TD-rRQzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 假设 vocab 是 NumPy 数组\n",
        "vocab_3 = np.array(['', '[UNK]', 'the', 'and', 'movie', 'this'])\n",
        "words = [\"movie\", \"the\", \"unknown_word\", \"this\"]\n",
        "\n",
        "# 构建字典\n",
        "vocab_dict = {word: idx for idx, word in enumerate(vocab_3)}\n",
        "print(vocab_dict)\n",
        "# 批量查找\n",
        "indices = [vocab_dict.get(word, 1) for word in words]\n",
        "print(\"单词索引:\", indices)  # 输出: [4, 2, 1, 5]\n",
        "\n",
        "# 反向查询（索引→单词）\n",
        "words_found = vocab_3[indices] # 通过索引列表，去反向访问单词\n",
        "print(\"对应单词:\", words_found)  # 输出: ['movie' 'the' '[UNK]' 'this']"
      ],
      "metadata": {
        "id": "1zgoU7knRQTF",
        "outputId": "da5163f3-c125-481b-c8f3-7b3eef6d26cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{np.str_(''): 0, np.str_('[UNK]'): 1, np.str_('the'): 2, np.str_('and'): 3, np.str_('movie'): 4, np.str_('this'): 5}\n",
            "单词索引: [4, 2, 1, 5]\n",
            "对应单词: ['movie' 'the' '[UNK]' 'this']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjId5pua3jHQ"
      },
      "source": [
        "Once the vocabulary is set, the layer can encode text into indices. The tensors of indices are 0-padded to the longest sequence in the batch (unless you set a fixed `output_sequence_length`):"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####encoder(example) 的运作\n",
        "encoder(example)\n",
        "作用：调用之前适配（adapt）过的 TextVectorization 层，将文本转换为 词索引序列。\n",
        "\n",
        "内部流程：\n",
        "\n",
        "* 标准化文本：自动转为小写、去除标点（根据 encoder 的配置）。\n",
        "\n",
        "* 分词：按空格拆分成单词列表（如 [\"i\", \"love\", \"this\", \"movie\"]）。\n",
        "\n",
        "* 词到索引映射：根据词汇表将每个词转换为对应的整数索引。\n",
        "\n",
        "* 若词在词汇表中 → 返回索引（如 \"movie\" → 4）。\n",
        "\n",
        "* 若词不在词汇表中 → 返回 [UNK] 的索引（默认是 1）。\n",
        "\n",
        "输出格式：返回一个 tf.Tensor，形状为 (序列长度,)，例如：\n",
        "\n",
        "* tf.Tensor([10, 25, 3, 4], shape=(4,), dtype=int64)  # 假设的映射结果"
      ],
      "metadata": {
        "id": "xItLHJxPX0Hc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "RGc7C9WiwRWs",
        "outputId": "408239b1-8388-4f32-c2db-854383df8e04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 674)\n",
            "(3, 674)\n",
            "(3, 674)\n",
            "64 <class 'tensorflow.python.framework.ops.EagerTensor'>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[10,  1, 16, ...,  0,  0,  0],\n",
              "       [11, 18,  7, ...,  0,  0,  0],\n",
              "       [ 8,  1,  4, ...,  0,  0,  0]])"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "encoded_example = encoder(example)[:3].numpy()\n",
        "print(encoder(example).shape)\n",
        "print(encoder(example)[:3].shape)\n",
        "print(encoded_example.shape)\n",
        "print(len(example), type(example))\n",
        "#print(len(encoded_example[0]),encoded_example[0])\n",
        "encoded_example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5cjz0bS39IN"
      },
      "source": [
        "With the default settings, the process is not completely reversible. There are three main reasons for that:\n",
        "\n",
        "1. The default value for `preprocessing.TextVectorization`'s `standardize` argument is `\"lower_and_strip_punctuation\"`.\n",
        "2. The limited vocabulary size and lack of character-based fallback results in some unknown tokens."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder(example)[0][0]"
      ],
      "metadata": {
        "id": "trmt4PY2ZRCj",
        "outputId": "7b03f075-1ab6-44cd-d007-40f47914e460",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=int64, numpy=10>"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example[n].numpy()[3]"
      ],
      "metadata": {
        "id": "u0YXryepZDiT",
        "outputId": "e158f844-a6e4-4bc2-de52-81ffd84cc0e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "49"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab[encoded_example[0][:10]] # 前10个字符"
      ],
      "metadata": {
        "id": "4Qutdt7OZ6lE",
        "outputId": "b1d6b8ce-b06e-48ff-ab74-e2205e17c776",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['i', '[UNK]', 'for', 'this', 'movie', 'for', 'years', 'apparently',\n",
              "       'it', '[UNK]'], dtype='<U14')"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(vocab))"
      ],
      "metadata": {
        "id": "oINjRmEVabZj",
        "outputId": "fc7e1d20-f9bb-4620-89d7-d02a36ba17e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "N_tD0QY5wXaK",
        "outputId": "b8e749cf-4233-4ad5-e609-db3160a17718",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  b\"I searched for this movie for years, apparently it ain't available here in the States so bought me a copy off Ebay.<br /><br />Four young hunters and three of their girlfriends venture into the woods searching for a bear that apparently has killed several campers. What they find is an ex-Vietnam vet gone crazy (he kills some of his victims using a glove with long metal finger nails a la Freddy Krueger). As soon as the night falls, one of the girls goes for a walk after a brief argument with her boyfriend, she gets killed. After one of the group finds her body, they all hide in their tents waiting for daylight. Once the sun comes up, all of them try and make it out, but fall victim one by one.<br /><br />Seven bodies, not a lot of gore, but a couple of good murders, especially the girls'deaths. The guys get killed in somewhat bloodless ways (blown up in car, shot to death, knife through head). <br /><br />Overall, INFERNAL TRAP is a nice slasher film from the late 80's. Nothing new, just well acted, fast paced and some pretty ladies. 10 out of 10.\"\n",
            "Round-trip:  i [UNK] for this movie for years apparently it [UNK] [UNK] here in the [UNK] so [UNK] me a [UNK] off [UNK] br four young [UNK] and three of their [UNK] [UNK] into the [UNK] [UNK] for a [UNK] that apparently has killed several [UNK] what they find is an [UNK] [UNK] gone crazy he [UNK] some of his [UNK] using a [UNK] with long [UNK] [UNK] [UNK] a la [UNK] [UNK] as soon as the night falls one of the girls goes for a [UNK] after a [UNK] [UNK] with her [UNK] she gets killed after one of the group finds her body they all [UNK] in their [UNK] [UNK] for [UNK] once the [UNK] comes up all of them try and make it out but fall [UNK] one by [UNK] br [UNK] [UNK] not a lot of gore but a couple of good [UNK] especially the [UNK] the guys get killed in somewhat [UNK] ways [UNK] up in car shot to death [UNK] through head br br overall [UNK] [UNK] is a nice [UNK] film from the late 80s nothing new just well [UNK] fast [UNK] and some pretty [UNK] 10 out of 10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
            "\n",
            "Original:  b\"This movie is not based on the bible. It completely leaves Christ out of the movie. They do not show the rapture or the second coming of Christ. Let alone talk about it. It does not quote from scriptures. The end times are called the great tribulation. The movie does not even show bad times. The seven bowls, seven viles and seven trumpets of judgements are boiled down to a 15 second news cast of the sea changing it's structure. The anti-Christ was killed 3 1/2 years into the tribulation and that is how the movie ended. The only part they got correct was there was two prophets. The did not use there names of course because that would be too close to the truth of scriptures. The worst part of it was I really wanted it to be a good movie. I wanted to take unsaved people to it. I feel that the movie is evil. It is a counterfeit just like everything the devil does. I just hope it does not take away from the upcoming movie based on the left behind books.<br /><br />The second problem with the movie is it was just bad. Bad acting, bad special effects, bad plot and poor character development. I have seen better episodes of Miami vice.\"\n",
            "Round-trip:  this movie is not based on the [UNK] it completely leaves [UNK] out of the movie they do not show the [UNK] or the second coming of [UNK] let alone talk about it it does not [UNK] from [UNK] the end times are called the great [UNK] the movie does not even show bad times the [UNK] [UNK] [UNK] [UNK] and [UNK] [UNK] of [UNK] are [UNK] down to a [UNK] second [UNK] cast of the [UNK] [UNK] its [UNK] the [UNK] was killed 3 [UNK] years into the [UNK] and that is how the movie [UNK] the only part they got [UNK] was there was two [UNK] the did not use there [UNK] of course because that would be too close to the truth of [UNK] the worst part of it was i really wanted it to be a good movie i wanted to take [UNK] people to it i feel that the movie is evil it is a [UNK] just like everything the [UNK] does i just hope it does not take away from the [UNK] movie based on the left behind [UNK] br the second problem with the movie is it was just bad bad acting bad special effects bad plot and poor character development i have seen better episodes of [UNK] [UNK]                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
            "\n",
            "Original:  b'In 1978 a phenomenon began. The release of John Carpenter\\'s \"Halloween\" got people queueing around the block to witness the evil that is Michael Meyers. The critics loved it, the world loved it, it was imitated, and has gone down as one of the greatest movies in cinematic history.<br /><br />plot: 15 years after a murder took place, four friends (all females) are babysitting(and having it on with their boyfriends) on Halloween night. After escaping from a hospital the night before, Michael myers Returns to his home town to stalk these people. He murders 3 of them silently and subtlety. He does not speak. He walks slowly. He hides....<br /><br />Only one of the friends escapes, after being saved by Doctor Loomis (Michael\\'s pursuer and doctor) <br /><br />There is one reason why Halloween works so well. Simplicity. We don\\'t know where Michael is, we don\\'t know why he kills, and he frightens us. They\\'re the only reasons why we are afraid.<br /><br />John carpenter wrote the movie, and directed. He builds unbearable tension throughout the story, and scares to such a degree, that sometimes we cannot watch. And the climax is truly startling.<br /><br />As horror, this is essential. It is terrifying and well acted. It is also mysterious. Michael is a force, not a human. A force that cannot be denied.<br /><br />The sequels focused too much around Michael and his \"history\". This movie focuses on the fear of the unknown. Perhaps that\\'s why this thing is a masterpiece.'\n",
            "Round-trip:  in [UNK] a [UNK] [UNK] the release of john [UNK] [UNK] got people [UNK] around the [UNK] to [UNK] the evil that is michael [UNK] the [UNK] loved it the world loved it it was [UNK] and has gone down as one of the greatest movies in [UNK] [UNK] br plot [UNK] years after a murder took place four friends all [UNK] are [UNK] having it on with their [UNK] on [UNK] night after [UNK] from a [UNK] the night before michael [UNK] [UNK] to his home town to [UNK] these people he [UNK] 3 of them [UNK] and [UNK] he does not [UNK] he [UNK] [UNK] he [UNK] br only one of the friends [UNK] after being [UNK] by [UNK] [UNK] [UNK] [UNK] and [UNK] br br there is one reason why [UNK] works so well [UNK] we dont know where michael is we dont know why he [UNK] and he [UNK] us theyre the only [UNK] why we are [UNK] br john [UNK] [UNK] the movie and directed he [UNK] [UNK] [UNK] throughout the story and [UNK] to such a [UNK] that sometimes we cannot watch and the [UNK] is truly [UNK] br as horror this is [UNK] it is [UNK] and well [UNK] it is also [UNK] michael is a [UNK] not a human a [UNK] that cannot be [UNK] br the [UNK] [UNK] too much around michael and his history this movie [UNK] on the [UNK] of the [UNK] perhaps thats why this thing is a [UNK]                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
            "\n"
          ]
        }
      ],
      "source": [
        "for n in range(3):\n",
        "  print(\"Original: \", example[n].numpy())\n",
        "  # 打印编码-解码后的文本（通过词汇表 vocab 转换回单词）。\n",
        "  print(\"Round-trip: \", \" \".join(vocab[encoded_example[n]])) #encoded_example[n]是一个索引数组\n",
        "  # \" \".join(...)\t将单词列表还原为连贯文本。\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjUqGVBxGw-t"
      },
      "source": [
        "## Create the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7zsmInBOCPO"
      },
      "source": [
        "![A drawing of the information flow in the model](https://github.com/tensorflow/text/blob/master/docs/tutorials/images/bidirectional.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgs6nnSTGw-t"
      },
      "source": [
        "Above is a diagram of the model.\n",
        "\n",
        "1. This model can be build as a `tf.keras.Sequential`.\n",
        "\n",
        "2. The first layer is the `encoder`, which converts the text to a sequence of token indices.\n",
        "\n",
        "3. After the encoder is an embedding layer. An embedding layer stores one vector per word. When called, it converts the sequences of word indices to sequences of vectors. These vectors are trainable. After training (on enough data), words with similar meanings often have similar vectors.\n",
        "\n",
        "  This index-lookup is much more efficient than the equivalent operation of passing a one-hot encoded vector through a `tf.keras.layers.Dense` layer.\n",
        "\n",
        "4. A recurrent neural network (RNN) processes sequence input by iterating through the elements. RNNs pass the outputs from one timestep to their input on the next timestep.\n",
        "\n",
        "  The `tf.keras.layers.Bidirectional` wrapper can also be used with an RNN layer. This propagates the input forward and backwards through the RNN layer and then concatenates the final output.\n",
        "\n",
        "  * The main advantage of a bidirectional RNN is that the signal from the beginning of the input doesn't need to be processed all the way through every timestep to affect the output.  \n",
        "\n",
        "  * The main disadvantage of a bidirectional RNN is that you can't efficiently stream predictions as words are being added to the end.\n",
        "\n",
        "5. After the RNN has converted the sequence to a single vector the two `layers.Dense` do some final processing, and convert from this vector representation to a single logit as the classification output.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4fodCI7soQi"
      },
      "source": [
        "The code to implement this is below:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    encoder, # 数据集\n",
        "    tf.keras.layers.Embedding(\n",
        "        input_dim=len(encoder.get_cocabulary()),\n",
        "        out_dim=64,\n",
        "        #这对于处理可变长度的序列非常重要。它告诉层在处理过程中忽略填充标记\n",
        "        #（用 0 表示）。填充通常用于使所有输入序列长度相同。\n",
        "        mask_zero=True),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        " '''\n",
        " 此代码定义了一个使用双向 LSTM 分析文本并预测其情感的文本分类模型。该模型使用 Keras 构建，包含一个嵌入层将单词表示为向量，\n",
        " 密集层进行额外处理，以及一个输出层提供最终的感性预测。\n",
        " '''"
      ],
      "metadata": {
        "id": "pyPnUc2ybfOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "LwfoBkmRYcP3"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    encoder,  #\n",
        "    tf.keras.layers.Embedding(\n",
        "        input_dim=len(encoder.get_vocabulary()),\n",
        "        output_dim=64,\n",
        "        # Use masking to handle the variable sequence lengths\n",
        "        mask_zero=True),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIGmIGkkouUb"
      },
      "source": [
        "Please note that Keras sequential model is used here since all the layers in the model only have single input and produce single output. In case you want to use stateful RNN layer, you might want to build your model with Keras functional API or model subclassing so that you can retrieve and reuse the RNN layer states. Please check [Keras RNN guide](https://www.tensorflow.org/guide/keras/rnn#rnn_state_reuse) for more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kF-PsCk1LwjY"
      },
      "source": [
        "The embedding layer [uses masking](https://www.tensorflow.org/guide/keras/masking_and_padding) to handle the varying sequence-lengths. All the layers after the `Embedding` support masking:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####这可以让您知道模型中哪些层能够处理掩码输入（在处理具有可变长度的数据序列时通常需要）。\n",
        "* 想象你有一组不同长度的句子。为了使它们的长度相同，你需要在较短的句子中添加填充。掩码就像一组指令，告诉模型：“不要关注填充，专注于句子中的实际单词。”\n",
        "* 通过忽略无关的填充标记，模型可以更有效地从输入数据的有效部分学习，从而提高准确率。\n",
        "*高效训练：掩码有助于防止在填充标记上进行不必要的计算，使训练过程更快更高效。\n",
        "* 该代码行 print([layer.supports_masking for layer in model.layers]) 很重要，因为它检查您的模型中的层是否能够利用此掩码机制。这确保了您的模型能够正确处理可变长度的序列，并从实际数据而不是填充数据中有效地学习。"
      ],
      "metadata": {
        "id": "5VCjZcqHi6f5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "87a8-CwfKebw",
        "outputId": "380fb40b-3fcb-4f69-c658-1f4d3cc4d0f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[False, True, True, True, True]\n"
          ]
        }
      ],
      "source": [
        "print([layer.supports_masking for layer in model.layers])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlS0iaUIWLpI"
      },
      "source": [
        "To confirm that this works as expected, evaluate a sentence twice. First, alone so there's no padding to mask:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "O41gw3KfWHus",
        "outputId": "3a792113-ba66-4300-a867-e72d6f44177a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'np' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ed52cb49a9f5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m sample_text = ('The movie was cool. The animation and the graphics '\n\u001b[1;32m      4\u001b[0m                'were out of this world. I would recommend this movie.')\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mencoded_sample_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencoded_sample_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ],
      "source": [
        "# predict on a sample text without padding.\n",
        "\n",
        "sample_text = ('The movie was cool. The animation and the graphics '\n",
        "               'were out of this world. I would recommend this movie.')\n",
        "print(np.array([sample_text]))\n",
        "encoded_sample_text = encoder(sample_text)\n",
        "print(np.array([encoded_sample_text]).shape)\n",
        "predictions = model.predict(np.array([encoded_sample_text]))\n",
        "print(predictions[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0VQmGnEWcuz"
      },
      "source": [
        "Now, evaluate it again in a batch with a longer sentence. The result should be identical:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "UIgpuTeFNDzq",
        "outputId": "d94a32fe-ff68-4b1b-d7fb-54ed52943208",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Invalid dtype: str256000",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-86-f28f728a572b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"the \"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optree/ops.py\u001b[0m in \u001b[0;36mtree_map\u001b[0;34m(func, tree, is_leaf, none_is_leaf, namespace, *rests)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0mleaves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreespec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_leaf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnone_is_leaf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0mflat_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mleaves\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtreespec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_up_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrests\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtreespec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mflat_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Invalid dtype: str256000"
          ]
        }
      ],
      "source": [
        "# predict on a sample text with padding\n",
        "\n",
        "padding = \"the \" * 2000\n",
        "predictions = model.predict(np.array([sample_text, padding]))\n",
        "print(predictions[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRI776ZcH3Tf"
      },
      "source": [
        "Compile the Keras model to configure the training process:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kj2xei41YZjC"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIwH3nto596k"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hw86wWS4YgR2"
      },
      "outputs": [],
      "source": [
        "history = model.fit(train_dataset, epochs=10,\n",
        "                    validation_data=test_dataset,\n",
        "                    validation_steps=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BaNbXi43YgUT"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = model.evaluate(test_dataset)\n",
        "\n",
        "print('Test Loss:', test_loss)\n",
        "print('Test Accuracy:', test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZmwt_mzaQJk"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plot_graphs(history, 'accuracy')\n",
        "plt.ylim(None, 1)\n",
        "plt.subplot(1, 2, 2)\n",
        "plot_graphs(history, 'loss')\n",
        "plt.ylim(0, None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwSE_386uhxD"
      },
      "source": [
        "Run a prediction on a new sentence:\n",
        "\n",
        "If the prediction is >= 0.0, it is positive else it is negative."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXgfQSgRW6zU"
      },
      "outputs": [],
      "source": [
        "sample_text = ('The movie was cool. The animation and the graphics '\n",
        "               'were out of this world. I would recommend this movie.')\n",
        "predictions = model.predict(np.array([sample_text]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7g1evcaRpTKm"
      },
      "source": [
        "## Stack two or more LSTM layers\n",
        "\n",
        "Keras recurrent layers have two available modes that are controlled by the `return_sequences` constructor argument:\n",
        "\n",
        "* If `False` it returns only the last output for each input sequence (a 2D tensor of shape (batch_size, output_features)). This is the default, used in the previous model.\n",
        "\n",
        "* If `True` the full sequences of successive outputs for each timestep is returned (a 3D tensor of shape `(batch_size, timesteps, output_features)`).\n",
        "\n",
        "Here is what the flow of information looks like with `return_sequences=True`:\n",
        "\n",
        "![layered_bidirectional](https://github.com/tensorflow/text/blob/master/docs/tutorials/images/layered_bidirectional.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbSClCrG1z8l"
      },
      "source": [
        "The interesting thing about using an `RNN` with `return_sequences=True` is that the output still has 3-axes, like the input, so it can be passed to another RNN layer, like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jo1jjO3vn0jo"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    encoder,\n",
        "    tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 64, mask_zero=True),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEPV5jVGp-is"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LeSE-YjdqAeN"
      },
      "outputs": [],
      "source": [
        "history = model.fit(train_dataset, epochs=10,\n",
        "                    validation_data=test_dataset,\n",
        "                    validation_steps=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LdwilM1qPM3"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = model.evaluate(test_dataset)\n",
        "\n",
        "print('Test Loss:', test_loss)\n",
        "print('Test Accuracy:', test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykUKnAoqbycW"
      },
      "outputs": [],
      "source": [
        "# predict on a sample text without padding.\n",
        "\n",
        "sample_text = ('The movie was not good. The animation and the graphics '\n",
        "               'were terrible. I would not recommend this movie.')\n",
        "predictions = model.predict(np.array([sample_text]))\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YYub0EDtwCu"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plot_graphs(history, 'accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plot_graphs(history, 'loss')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xvpE3BaGw_V"
      },
      "source": [
        "Check out other existing recurrent layers such as [GRU layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU).\n",
        "\n",
        "If you're interested in building custom RNNs, see the [Keras RNN Guide](https://www.tensorflow.org/guide/keras/rnn).\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "text_classification_rnn.ipynb",
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}